import{_ as i,K as e,L as r,a5 as o,W as n,F as s}from"./framework-edebdfe1.js";const t={},a=n('<h1 id="llm" tabindex="-1"><a class="header-anchor" href="#llm" aria-hidden="true">#</a> LLM</h1><ol><li><p><strong>数据准备</strong></p><ul><li>收集语料（通用文本、领域文本、指令数据）</li><li>数据清洗（去重、脱敏、格式化）</li><li>划分数据集（Pretrain / SFT / Eval）</li></ul></li><li><p><strong>Tokenizer 训练</strong></p><ul><li>训练 BPE / SentencePiece / WordPiece</li><li>生成 vocab 和 model 文件</li><li>测试 tokenizer 正确性</li></ul></li><li><p><strong>预训练（Pretrain）</strong></p><ul><li>初始化 Transformer 模型权重（随机或加载预训练）</li><li>配置模型参数（层数、hidden size、seq_len）</li><li>数据加载与 batching</li><li>模型训练（CrossEntropy Loss，自监督）</li><li>保存中间和最终权重</li></ul></li><li><p><strong>监督微调（SFT）</strong></p><ul><li>使用指令数据或人类示范数据</li><li>仅微调输出头或全模型</li><li>CrossEntropy Loss 对 assistant 输出计算</li><li>保存 SFT 权重</li></ul></li><li><p><strong>奖励模型训练（可选）</strong></p><ul><li>收集对比数据（好/差回答）</li><li>训练 reward 模型预测人类偏好</li><li>用于 RLHF / DPO</li></ul></li><li><p><strong>强化学习微调（RLHF / DPO / PPO）</strong></p><ul><li>加载 SFT 模型作为 reference policy</li><li>通过 reward 最大化进行微调</li><li>KL 正则约束模型不偏离 SFT</li><li>保存 RL 微调权重</li></ul></li><li><p><strong>评估与测试</strong></p><ul><li>指令遵循测试</li><li>对话质量评估</li><li>安全和拒答测试</li><li>性能和推理速度测试</li></ul></li><li><p><strong>部署</strong></p><ul><li>转换模型格式（GGUF / ONNX / TorchScript）</li><li>构建推理接口（API / Web UI）</li><li>上线监控和日志采集</li></ul></li></ol>',2);function c(p,u){const l=s("DocsAD");return e(),r("div",null,[a,o(l)])}const g=i(t,[["render",c],["__file","llm.html.vue"]]);export{g as default};
