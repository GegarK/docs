import{_ as s,K as l,L as i,a5 as t,M as r,N as e,W as h,F as n}from"./framework-edebdfe1.js";const c={},d=r("h1",{id:"transformer",tabindex:"-1"},[r("a",{class:"header-anchor",href:"#transformer","aria-hidden":"true"},"#"),e(" Transformer")],-1),f=r("h1",{id:"什么是-transformer",tabindex:"-1"},[r("a",{class:"header-anchor",href:"#什么是-transformer","aria-hidden":"true"},"#"),e(" 什么是 Transformer")],-1),m=r("p",null,[e("Transformer 是一种"),r("strong",null,[e("基于注意力机制（Attention）"),r("strong",null,[e("的深度学习模型架构，最早由 Google 在 2017 年论文 "),r("strong",null,"《Attention Is All You Need》"),e(" 中提出。它彻底改变了自然语言处理（NLP），并成为当今大模型（如 GPT、BERT、Claude 等）的")]),e("基础算法框架")]),e("。")],-1),p=h('<p>一句话定义：</p><blockquote><p><strong>Transformer 是一种通过自注意力机制，对序列数据进行全局建模、并行计算的神经网络架构。</strong></p></blockquote><hr><h2 id="一、transformer-解决了什么问题" tabindex="-1"><a class="header-anchor" href="#一、transformer-解决了什么问题" aria-hidden="true">#</a> 一、Transformer 解决了什么问题？</h2><p>在 Transformer 之前，序列建模主要依赖：</p><ul><li>RNN / LSTM / GRU</li></ul><p>这些模型存在明显瓶颈：</p><ul><li>❌ <strong>无法并行计算</strong>（必须按时间步顺序处理）</li><li>❌ <strong>长距离依赖困难</strong>（梯度消失 / 记忆衰减）</li><li>❌ <strong>训练效率低、难以规模化</strong></li></ul><p>Transformer 的核心突破是：</p><blockquote><p><strong>完全抛弃递归结构，用注意力机制直接建模任意位置之间的关系</strong>。</p></blockquote><hr><h2 id="二、transformer-的核心思想" tabindex="-1"><a class="header-anchor" href="#二、transformer-的核心思想" aria-hidden="true">#</a> 二、Transformer 的核心思想</h2><h3 id="自注意力-self-attention" tabindex="-1"><a class="header-anchor" href="#自注意力-self-attention" aria-hidden="true">#</a> 自注意力（Self-Attention）</h3><p>自注意力机制的本质是：</p><blockquote><p><strong>序列中的每一个元素，都可以“关注”序列中的所有其他元素，并根据相关性动态加权。</strong></p></blockquote><p>例如在句子中：</p><blockquote><p>“The animal didn’t cross the street because <strong>it</strong> was too tired.”</p></blockquote><p>模型需要知道 <strong>it</strong> 指代的是 <em>animal</em>，而不是 <em>street</em>。</p><p>Self-Attention 正是为了解决这种<strong>全局依赖建模</strong>问题。</p><h3 id="可视化" tabindex="-1"><a class="header-anchor" href="#可视化" aria-hidden="true">#</a> 可视化</h3>',20),u={href:"https://bbycroft.net/llm",target:"_blank",rel:"noopener noreferrer"},_={href:"https://poloclub.github.io/transformer-explainer/",target:"_blank",rel:"noopener noreferrer"},b=r("hr",null,null,-1),g=r("h2",{id:"transformer-算法讲解",tabindex:"-1"},[r("a",{class:"header-anchor",href:"#transformer-算法讲解","aria-hidden":"true"},"#"),e(" Transformer 算法讲解")],-1),x=r("h2",{id:"transformer-代码开发",tabindex:"-1"},[r("a",{class:"header-anchor",href:"#transformer-代码开发","aria-hidden":"true"},"#"),e(" Transformer 代码开发")],-1);function T(k,N){const a=n("DocsAD"),o=n("ExternalLinkIcon");return l(),i("div",null,[d,f,m,t(a),p,r("ul",null,[r("li",null,[r("a",u,[e("https://bbycroft.net/llm"),t(o)])]),r("li",null,[r("a",_,[e("https://poloclub.github.io/transformer-explainer/"),t(o)])])]),b,g,x])}const A=s(c,[["render",T],["__file","transformer.html.vue"]]);export{A as default};
